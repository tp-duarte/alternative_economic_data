{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_into_df(query, cursor):\n",
    "\t\"\"\"\n",
    "\n",
    "\tDescription: This function loads a SQL query into a pandas DataFrame.\n",
    "\n",
    "\tParameters:\n",
    "\n",
    "\ttable_name (str): A string containing the name of the table to be retrieved;\n",
    "\tcursor (database cursor): A cursor to make a connection with the database.\n",
    "\n",
    "\t\"\"\"\n",
    "\n",
    "\tcursor.execute(query)\n",
    "\tresult = cursor.fetchall()\n",
    "\tcolumn_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "\tdf = pd.DataFrame(result, columns=column_names)\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_suffix_to_duplicates(lst):\n",
    "    frequency = {}\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if item in frequency:\n",
    "            frequency[item] += 1\n",
    "            item_with_suffix = item + \"_dupplicated_\" + str(frequency[item])\n",
    "            result.append(item_with_suffix)\n",
    "        else:\n",
    "            frequency[item] = 0\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "my_list = ['apple', 'orange', 'banana', 'apple', 'apple', 'banana']\n",
    "suffix = '_dup'\n",
    "result_list = add_suffix_to_duplicates(my_list,)\n",
    "print(result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_into_h2o(query, cursor):\n",
    "    \"\"\"\n",
    "\n",
    "    Description: This function loads a SQL query into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    table_name (str): A string containing the name of the table to be retrieved;\n",
    "    cursor (database cursor): A cursor to make a connection with the database.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    column_names = add_suffix_to_duplicates(column_names)\n",
    "\n",
    "    df = h2o.H2OFrame(result, column_names=column_names)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(split_time, df, drop_from_features, target):\n",
    "    \"\"\"\n",
    "    Description: Splits data into training and test considerind a split date.\n",
    "    \n",
    "    Parameters:\n",
    "        split_time (str): A string with the split date in Y-m-d format;\n",
    "        df (h2o.H2OFrame): A h2o frame containing the data to be splitted;\n",
    "        drop_from_features (list): A list containing the features (and the target) to be dropped out of the training step;\n",
    "        target (str): The name of the target variable;\n",
    "    \n",
    "    Returns:\n",
    "        train (h2o.H2OFrame): The data for train;\n",
    "        test (h2o.H2OFrame): The data for test;\n",
    "        features (list): A list of features to predict the target;\n",
    "        target (str): A string of the target to predicted;\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    split_time = pd.to_datetime(split_time)\n",
    "    \n",
    "    train = df[df[\"date\"] <  split_time]\n",
    "    test = df[df[\"date\"] >=  split_time]\n",
    "    \n",
    "    features = train.columns\n",
    "    features = [feature_col for feature_col in features if feature_col not in drop_from_features]\n",
    "    \n",
    "    return train, test, features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting models metrics and results\n",
    "\n",
    "def calculate_metrics(actual_frame, predicted_frame):\n",
    "    \"\"\"\n",
    "    Calculate RMSE, MAE, and R-squared metrics.\n",
    "\n",
    "    Parameters:\n",
    "        actual_frame (H2OFrame): The actual target values;\n",
    "        predicted_frame (H2OFrame): The predicted target values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the calculated metrics.\n",
    "        \n",
    "    \"\"\"\n",
    "    actual_values = actual_frame.as_data_frame().values.flatten()\n",
    "    predicted_values = predicted_frame.as_data_frame().values.flatten()\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))\n",
    "    mae = mean_absolute_error(actual_values, predicted_values)\n",
    "    r2 = r2_score(actual_values, predicted_values)\n",
    "\n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R-squared\": r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(models, actual_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Display the performance metrics for each model in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    models (list): A list of model names.\n",
    "    actual_values (H2OFrame): H2OFrame with the actual target values.\n",
    "    predicted_values (list): A list of H2OFrames with the predicted target values for each model.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the performance metrics for each model.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    actual_values = [actual_values]\n",
    "    actual_values = len(models) * actual_values\n",
    "    \n",
    "    for model_name, actual, predicted in zip(models, actual_values, predicted_values):\n",
    "        \n",
    "        metrics = calculate_metrics(actual, predicted)\n",
    "        results[model_name] = metrics\n",
    "\n",
    "    df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quarterly_series(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Description: This function generates a quarterly time series between the specified start and end dates.\n",
    "\n",
    "    Parameters:\n",
    "        start_date (str): A string representing the start date in the format 'YYYY-MM-DD'.\n",
    "        end_date (str): A string representing the end date in the format 'YYYY-MM-DD'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.Series: A series object containing the quarterly time series.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a DatetimeIndex with quarterly frequency\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='QS')\n",
    "\n",
    "    # Create a pandas Series with the quarterly time series\n",
    "    quarterly_series = pd.Series(dates)\n",
    "\n",
    "    return quarterly_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FOLDER = r\"alternative_economic_data\\data\\ml_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = psycopg2.connect(\n",
    "        host=\"aws-california.caclwjj7hnoo.us-east-2.rds.amazonaws.com\",\n",
    "        database=\"california\",\n",
    "        user=\"postgres\",\n",
    "        password=\"24567811\"\n",
    "    )\n",
    "    cursor = db.cursor()\n",
    "    print(\"Connected to the database!\")\n",
    "    \n",
    "    # Perform database operations here\n",
    "    \n",
    "except psycopg2.Error as e:\n",
    "    print(\"Error connecting to the database:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_trad_data = \"SELECT trad.* \" \\\n",
    "                  \"FROM traditional_data AS trad\"\n",
    "                 \n",
    "query_alt_data = \"SELECT alt.*, trad.gdp \" \\\n",
    "                 \"FROM traditional_data AS trad \" \\\n",
    "                 \"JOIN alternative_data AS alt ON trad.date = alt.date\"\n",
    "\n",
    "query_all_data = \"SELECT trad.*, alt.* \" \\\n",
    "                 \"FROM traditional_data AS trad \" \\\n",
    "                 \"FULL OUTER JOIN alternative_data AS alt \" \\\n",
    "                 \"ON trad.date = alt.date\"\n",
    "\n",
    "traditional_data = query_into_h2o(query_trad_data, cursor)\n",
    "alternative_data = query_into_h2o(query_alt_data, cursor)\n",
    "all_data = query_into_h2o(query_all_data, cursor)\n",
    "all_data = all_data.drop(\"date_dupplicated_1\")\n",
    "\n",
    "db.close() # don't want to kill your finances in aws haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data frames \n",
    "\n",
    "traditional_data = traditional_data.sort(by='date')\n",
    "alternative_data = alternative_data.sort(by='date')\n",
    "all_data = all_data.sort(by='date')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "### Predicting Expenditure and disposable Income \n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expenditure:\n",
    "\n",
    "- Remove GDP and disp_inc\n",
    "\n",
    "Disp Income:\n",
    "\n",
    "- Remove GDP and expenditure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and test data based on split time and columns to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof, tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_tracker = tracker.SummaryTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |\n",
      "13:59:29.912: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "\n",
      "13:59:30.266: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 56.0.\n",
      "\n",
      "███████████████████████████████████████████████████████████████| (done) 100%\n",
      "gbm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "                               types |   # objects |   total size\n",
      "==================================== | =========== | ============\n",
      "                                list |       23479 |      1.98 MB\n",
      "                                 str |       25230 |      1.81 MB\n",
      "  h2o.backend.connection.H2OResponse |         677 |    192.36 KB\n",
      "                                 int |        5704 |    156.90 KB\n",
      "                               tuple |         418 |     42.76 KB\n",
      "                                dict |          80 |     16.61 KB\n",
      "                                type |          14 |      5.58 KB\n",
      "                                code |          -4 |      4.31 KB\n",
      "                               bytes |          37 |      3.73 KB\n",
      "                  wrapper_descriptor |          28 |      1.97 KB\n",
      "                             weakref |          16 |      1.12 KB\n",
      "                            ast.Name |          19 |    912     B\n",
      "                        ast.Constant |           6 |    288     B\n",
      "                            ast.Call |           4 |    192     B\n",
      "                         ast.keyword |           4 |    192     B\n"
     ]
    }
   ],
   "source": [
    "## Expenditure data\n",
    "\n",
    "split_date = \"2019-01-01\"\n",
    "drop_from_expend = [\"gdp\", \"disposable_income\", \"expenditure\"]\n",
    "\n",
    "train_expend, test_expend, features_expend, expend = data_split(split_date, traditional_data, drop_from_expend, \"expenditure\")\n",
    "\n",
    "# Initializing and training model \n",
    "\n",
    "expendt_model = H2OAutoML(max_runtime_secs=60)\n",
    "expendt_model.train(x=features_expend, y=expend, training_frame=train_expend)\n",
    "\n",
    "# Predictions  \n",
    "\n",
    "predictions_expend = expendt_model.predict(test_expend[features_expend])\n",
    "del expendt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disposable Income \n",
    "\n",
    "drop_from_disp = [\"gdp\", \"disposable_income\", \"expenditure\"]\n",
    "\n",
    "train_disp, test_disp, features_disp, disp = data_split(split_date, traditional_data, drop_from_disp, \"disposable_income\")\n",
    "\n",
    "# Initializing and training model \n",
    "\n",
    "disp_model = H2OAutoML(max_runtime_secs=10)\n",
    "disp_model.train(x=features_disp, y=disp, training_frame=train_disp)\n",
    "\n",
    "# Predictions  \n",
    "\n",
    "predictions_disp = disp_model.predict(test_disp[features_disp])\n",
    "del disp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traditional Data\n",
    "\n",
    "\n",
    "drop_from_trad = [\"gdp\"]\n",
    "\n",
    "train_trad, test_trad, features_trad, gdp = data_split(split_date, traditional_data, drop_from_trad, \"gdp\")\n",
    "\n",
    "# replacing expenditure and tradosable income data in the test \n",
    "\n",
    "test_trad[\"expenditure\"] = predictions_expend\n",
    "test_trad[\"disposable_income\"] = predictions_disp\n",
    "\n",
    "# Initializing and training model \n",
    "\n",
    "trad_model = H2OAutoML(max_runtime_secs=10)\n",
    "trad_model.train(x=features_trad, y=gdp, training_frame=train_trad)\n",
    "\n",
    "# Predictions  \n",
    "\n",
    "predictions_trad = trad_model.predict(test_trad[features_trad])\n",
    "del disp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative Data\n",
    "\n",
    "\n",
    "drop_from_alt = [\"gdp\"]\n",
    "\n",
    "train_alt, test_alt, features_alt, gdp = data_split(split_date, alternative_data, drop_from_alt, \"gdp\")\n",
    "\n",
    "\n",
    "# Initializing and training model \n",
    "\n",
    "alt_model = H2OAutoML(max_runtime_secs=10)\n",
    "alt_model.train(x=features_alt, y=gdp, training_frame=train_alt)\n",
    "\n",
    "# Predictions  \n",
    "\n",
    "predictions_alt = alt_model.predict(test_alt[features_alt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All Data\n",
    "\n",
    "drop_from_all = [\"gdp\"]\n",
    "\n",
    "train_all, test_all, features_all, gdp = data_split(split_date, all_data, drop_from_all, \"gdp\")\n",
    "\n",
    "# replacing expenditure and disposable income data in the test \n",
    "\n",
    "test_all[\"expenditure\"] = predictions_expend\n",
    "test_all[\"disposable_income\"] = predictions_disp\n",
    "\n",
    "# Initializing and training model \n",
    "\n",
    "all_model = H2OAutoML(max_runtime_secs=10)\n",
    "all_model.train(x=features_all, y=gdp, training_frame=train_all)\n",
    "\n",
    "# Predictions  \n",
    "\n",
    "predictions_all = all_model.predict(test_all[features_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"trad_model\", \"alt_model\", \"all_model\"]\n",
    "predictions = [predictions_trad, predictions_alt, predictions_all]\n",
    "actual_gdp = test_all[\"gdp\"]\n",
    "\n",
    "models_metrics = display_metrics(models, actual_gdp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flattened_dict = {\n",
    "    \"date\": generate_quarterly_series(split_date, \"2022-10-01\"),        \n",
    "    \"real_gdp\": test_all[\"gdp\"].as_data_frame(),\n",
    "    \"trad_predictions\": predictions_trad.as_data_frame(),\n",
    "    \"alt_predictions\": predictions_alt.as_data_frame(),\n",
    "    \"all_predictions\": predictions_all.as_data_frame()\n",
    "}\n",
    "\n",
    "prediction_results = pd.concat(flattened_dict.values(), axis=1, keys=flattened_dict.keys())\n",
    "prediction_results.columns = prediction_results .columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_file = \"..\\data\\ml_data\\predictions.csv\"\n",
    "metrics_file = \"..\\data\\ml_data\\models_metrics.csv\"\n",
    "\n",
    "prediction_results.to_csv(prediction_file, index=False)\n",
    "models_metrics.to_csv(metrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
